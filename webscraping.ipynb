{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is used to scrape links from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# collect information from a single card\n",
    "def collect_card_info(card):\n",
    "    link = card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "    headline =  card.find_element(By.CLASS_NAME, 'container__headline-text').text\n",
    "    date = card.find_element(By.CLASS_NAME, 'container__date').text\n",
    "    description = card.find_element(By.CLASS_NAME, 'container__description').text\n",
    "    return link, headline, date, description\n",
    "\n",
    "# collect information from all cards on a page\n",
    "def collect_page_info(driver):\n",
    "    info = []\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-component-name=\"card\"]')\n",
    "    for card in cards:\n",
    "        info.append(collect_card_info(card))\n",
    "    return info\n",
    "\n",
    "url_template = 'https://www.cnn.com/search?q=trump&from={}&size=10&page={}&sort=newest&types=article&section=Stat133Final+Project'\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "info_collected = 0\n",
    "page_number = 1\n",
    "\n",
    "with open('cnn.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Link', 'Headline', 'Date', 'Description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    while info_collected < 1000:\n",
    "        url = url_template.format((page_number - 1) * 10, page_number)\n",
    "        driver.get(url)\n",
    "        time.sleep(20)\n",
    "        page_info = collect_page_info(driver)\n",
    "        \n",
    "        if not page_info:\n",
    "            print(\"No more cards found on page\", page_number)\n",
    "            break\n",
    "        \n",
    "        for link, headline, date, description in page_info:\n",
    "            writer.writerow({'Link': link, 'Headline': headline, 'Date': date, 'Description': description})\n",
    "            info_collected += 1\n",
    "            #print(\"Info collected:\", info_collected)\n",
    "            if info_collected >= 1000:\n",
    "                break\n",
    "        \n",
    "        page_number += 1\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below was used to collect links from NY Post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os\n",
    "\n",
    "# collect information from a single card\n",
    "def collect_card_info(card):\n",
    "    link = card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "    headline =  card.find_element(By.CLASS_NAME, 'story__headline').text\n",
    "    date_element = card.find_element(By.CLASS_NAME, 'meta--byline')\n",
    "    date = date_element.text.split('|')[0].strip() # grabs the author and the date\n",
    "    description = card.find_element(By.CLASS_NAME, 'story__excerpt').text\n",
    "    return link, headline, date, description\n",
    "\n",
    "# collect information from all cards on a page\n",
    "def collect_page_info(driver):\n",
    "    info = []\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, 'div.search-results__story')\n",
    "    for card in cards:\n",
    "        info.append(collect_card_info(card))\n",
    "    return info\n",
    "\n",
    "url_template = 'https://nypost.com/search/trump/'\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "info_collected = 0\n",
    "page_number = 1\n",
    "\n",
    "\n",
    "with open('nypost.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Link', 'Headline', 'Date', 'Description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    while info_collected < 1000:\n",
    "        # your existing code\n",
    "        url = url_template.format((page_number - 1) * 10, page_number)\n",
    "        driver.get(url)\n",
    "        time.sleep(20)\n",
    "        page_info = collect_page_info(driver)\n",
    "        \n",
    "        if not page_info:\n",
    "            print(\"No more cards found on page\", page_number)\n",
    "            break\n",
    "        \n",
    "        for link, headline, date, description in page_info:\n",
    "            writer.writerow({'Link': link, 'Headline': headline, 'Date': date, 'Description': description})\n",
    "            info_collected += 1\n",
    "            #print(\"Info collected:\", info_collected)\n",
    "            if info_collected >= 1000:\n",
    "                break\n",
    "        \n",
    "        page_number += 1\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
